{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye8QGEeGNnGj"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we cover the process of training the best possible model to classify MRI images of brain tumours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Modules\n",
    "We first start by importing all the modules we need to proceed with this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api._v2.keras.applications.resnet import ResNet50\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "Model = tf.keras.models.Model\n",
    "VGG19 = tf.keras.applications.VGG19\n",
    "ResNet50 = tf.keras.applications.ResNet50\n",
    "\n",
    "GlobalAveragePooling2D = tf.keras.layers.GlobalAveragePooling2D\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Global Constants\n",
    "We need to define some global constants to keep things consistent across the notebook. This will allow us to use the same constant that could contain different values based on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since running this in the local computer takes a huge amount of time behind hardware acceleration, we have two configurations that we can load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20444,
     "status": "ok",
     "timestamp": 1680151007839,
     "user": {
      "displayName": "Juan Carlos Wu (Jc)",
      "userId": "07576149222644747820"
     },
     "user_tz": 240
    },
    "id": "r-Q6j_Rllhx9",
    "outputId": "de2a1b9a-7591-4b68-9c27-f393d5afbc81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../\n",
      "../data/raw\n",
      "../data/raw\\Training\n",
      "../data/raw\\Testing\n",
      "../trained-models\n"
     ]
    }
   ],
   "source": [
    "if ON_COLAB:\n",
    "    # mound google drive\n",
    "    from google.colab import drive\n",
    "\n",
    "    DRIVE_ROOT = \"/content/drive\"\n",
    "    DRIVE_DATA_ROOT = os.path.join(drive_root, \"My Drive/CP322\")\n",
    "    DRIVE_TRAINING_FOLDER = os.path.join(drive_data_root, \"Training\")\n",
    "    DRIVE_TESTING_FOLDER = os.path.join(drive_data_root, \"Testing\")\n",
    "\n",
    "    drive.mount(drive_root)\n",
    "\n",
    "    MODEL_SAVE_PATH = os.path.join(drive_data_root)\n",
    "    \n",
    "else:\n",
    "    DRIVE_ROOT = \"../\"\n",
    "    DRIVE_DATA_ROOT = os.path.join(drive_root, \"data/raw\")\n",
    "    DRIVE_TRAINING_FOLDER = os.path.join(drive_data_root, \"Training\")\n",
    "    DRIVE_TESTING_FOLDER = os.path.join(drive_data_root, \"Testing\")\n",
    "    \n",
    "    MODEL_SAVE_PATH = os.path.join(drive_root, 'trained-models')\n",
    "\n",
    "print(DRIVE_ROOT)\n",
    "print(DRIVE_DATA_ROOT)\n",
    "print(DRIVE_TRAINING_FOLDER)\n",
    "print(DRIVE_TESTING_FOLDER)\n",
    "print(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Data Processing Methods\n",
    "Here we define the methods used to load data and pre-process raw data. This will be useful down the line when doing multiple adjustments and/or loading different set of data to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1680151007841,
     "user": {
      "displayName": "Juan Carlos Wu (Jc)",
      "userId": "07576149222644747820"
     },
     "user_tz": 240
    },
    "id": "rNcXLuzCn7LS"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "  assert type(target_size) == tuple, \"Target size must be of type tuple\"\n",
    "  img = tf.io.read_file(img_path)\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  img = tf.image.resize(img, target_size)\n",
    "  img /= 255.0 # normalize\n",
    "  return img\n",
    "\n",
    "def load_mixed_data(target=\"train\", test_size=0.2, random_state=42, limit=200):\n",
    "  # for now the method only loads all the data as yes or no\n",
    "\n",
    "  X = []\n",
    "  Y = []\n",
    "  # load images that don't have brain tumour\n",
    "  if target == \"train\":\n",
    "    data_dir = DRIVE_TRAINING_FOLDER\n",
    "  else:\n",
    "    data_dir = DRIVE_TESTING_FOLDER\n",
    "  \n",
    "  no_dir = os.path.join(data_dir, \"no\")\n",
    "  count = 0\n",
    "  for img_name in os.listdir(no_dir):\n",
    "    if count == limit:\n",
    "      break\n",
    "    img = preprocess_image(os.path.join(no_dir, img_name))\n",
    "    X.append(img)\n",
    "    Y.append(0)\n",
    "    count += 1\n",
    "  \n",
    "  # load images that have brain tumour\n",
    "  labels = ['pituitary', 'meningioma', 'glioma']\n",
    "  for label in labels:\n",
    "    count = 0\n",
    "    folder = os.path.join(data_dir, label)\n",
    "    for img_name in os.listdir(folder):\n",
    "      if count == limit:\n",
    "        break\n",
    "      img = preprocess_image(os.path.join(folder, img_name))\n",
    "      X.append(img)\n",
    "      Y.append(1)\n",
    "      count += 1\n",
    "  \n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y)\n",
    "\n",
    "  return train_test_split(X, Y, test_size=test_size, random_state=random_state)  \n",
    "\n",
    "def load_petuitary_data(target=\"train\", test_size=0.2, random_state=42):\n",
    "  pass\n",
    "\n",
    "def load_meningioma_data(target=\"train\", test_size=0.2, random_state=42):\n",
    "  pass\n",
    "\n",
    "def load_glioma_data(target=\"train\", test_size=0.2, random_state=42):\n",
    "  pass\n",
    "\n",
    "def load_general_data(target=\"train\", test_size=0.2, random_state=42):\n",
    "  pass\n",
    "\n",
    "def load_data(data_type=\"mixed\", target=\"train\", test_size=0.2, random_state=42, limit=200):\n",
    "  # 4 different types of data\n",
    "  # 1. mixed (pituitary, meningioma, and glioma, general brain tumour)\n",
    "  # 2. pituitary\n",
    "  # 3. meningioma\n",
    "  # 4. glioma\n",
    "  # 5. general (small data set of no specified type of brain tumour)\n",
    "\n",
    "  # 2 different targets\n",
    "  # 1. train -> loads training data\n",
    "  # 2. test -> loads test data\n",
    "\n",
    "  if data_type == \"mixed\":\n",
    "    return load_mixed_data(target, test_size, random_state, limit)\n",
    "  elif data_type == \"pituitary\":\n",
    "    return load_petuitary_data(target, test_size, random_state)\n",
    "  elif data_type == \"meningioma\":\n",
    "    return load_meningioma_data(target, test_size, random_state)\n",
    "  elif data_type == \"glioma\":\n",
    "    return load_glioma_data(target, test_size, random_state)\n",
    "  elif data_type == \"general\":\n",
    "    return load_general_data(target, test_size, random_state)\n",
    "  \n",
    "  raise ValueError(f\"Data type must be of the following: 'mixed', 'petuitary', 'meningioma', 'glioma', or 'general'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Models\n",
    "\n",
    "We will use two pre-trained models VGG19 and ResNet50 as based models. This will greatly reduce the time to train a good predictor model.\n",
    "\n",
    "We will adjust the models to better fit our need, which is to classify MRI images of brains as have tumour or no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1680151007842,
     "user": {
      "displayName": "Juan Carlos Wu (Jc)",
      "userId": "07576149222644747820"
     },
     "user_tz": 240
    },
    "id": "b0F1VFhZy2BJ"
   },
   "outputs": [],
   "source": [
    "class DetectorModelBaseVGG19:\n",
    "    def __init__(self, weights='imagenet', include_top=False, input_shape=(224, 224, 3), classes=[\"no\", \"yes\"]):\n",
    "        self.vgg19 = VGG19(weights=weights, include_top=include_top, input_shape=input_shape)\n",
    "        self.classes = [cl for cl in classes] \n",
    "        self.model = None\n",
    "        self.custom_layers = []\n",
    "        \n",
    "    def build(self):\n",
    "        for layer in self.vgg19.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        x = self.vgg19.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        # sigmoid is logistic function, suited for binary classification\n",
    "        predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "        self.model = Model(inputs=self.vgg19.input, outputs=predictions)\n",
    "\n",
    "    \n",
    "    def fit(self, train_X, train_Y, test_X, test_Y, batch_size=32, epochs=30, verbose=1):\n",
    "        train_Y = to_categorical(train_Y, num_classes=len(self.classes))\n",
    "        test_Y = to_categorical(test_Y, num_classes=len(self.classes))\n",
    "        history = self.model.fit(\n",
    "                    train_X,\n",
    "                    train_Y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=verbose,\n",
    "                    validation_data=(test_X, test_Y))\n",
    "\n",
    "        return history\n",
    "\n",
    "    def compile(self, optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]):\n",
    "        self.model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=loss,\n",
    "                metrics=metrics\n",
    "                )\n",
    "\n",
    "    def save(self, save_path):\n",
    "        self.model.save(save_path)\n",
    "\n",
    "    def score(self, test_X, test_Y, verbose=0):\n",
    "        test_Y = to_categorical(test_Y, num_classes=len(self.classes))\n",
    "        return self.model.evaluate(test_X, test_Y, verbose=verbose)\n",
    "\n",
    "    def grid_search(self, param_grid, train_X, train_Y, test_X, test_Y, epochs=30, verbose=1):\n",
    "        best_model = None\n",
    "        best_score = -np.inf\n",
    "        best_params = None\n",
    "        best_history = None\n",
    "\n",
    "        param_combinations = ParameterGrid(param_grid)\n",
    "\n",
    "        for params in param_combinations:\n",
    "            print(f\"Training with parameters: {params}\")\n",
    "\n",
    "            # Reset model\n",
    "            self.build()\n",
    "            self.compile(optimizer=params['optimizer'], loss=params['loss'], metrics=[\"accuracy\"])\n",
    "\n",
    "            history = self.fit(train_X, train_Y, test_X, test_Y, batch_size=params['batch_size'], epochs=epochs, verbose=verbose)\n",
    "\n",
    "            score = self.score(test_X, test_Y)\n",
    "\n",
    "            if score[1] > best_score:\n",
    "                best_model = self.model\n",
    "                best_score = score[1]\n",
    "                best_params = params\n",
    "                best_history = history\n",
    "\n",
    "            print(f\"Accuracy: {score[1]}\")\n",
    "\n",
    "        self.model = best_model\n",
    "\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best accuracy: {best_score}\")\n",
    "\n",
    "        return best_history, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1680151007843,
     "user": {
      "displayName": "Juan Carlos Wu (Jc)",
      "userId": "07576149222644747820"
     },
     "user_tz": 240
    },
    "id": "Q3slXzTkHeBU"
   },
   "outputs": [],
   "source": [
    "class DetectorModelBaseResNet:\n",
    "    def __init__(self, input_shape=(224, 224, 3)):\n",
    "        self.input_shape = input_shape\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        resnet50_base = ResNet50(weights='imagenet', include_top=False,\n",
    "                                                       input_shape=self.input_shape)\n",
    "        x1 = resnet50_base.output\n",
    "        x1 = tf.keras.layers.GlobalAveragePooling2D()(x1)\n",
    "        x1 = tf.keras.layers.Dense(1024, activation='relu')(x1)\n",
    "        x1 = tf.keras.layers.Dropout(0.5)(x1)\n",
    "        predictions = tf.keras.layers.Dense(2, activation='softmax')(x1)\n",
    "\n",
    "        self.model = Model(inputs=[resnet50_base.input], outputs=[predictions])\n",
    "\n",
    "        for layer in resnet50_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        self.model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def train(self, train_X, train_Y, batch_size=32, epochs=30, validation_split=0.2):\n",
    "        train_Y = to_categorical(train_Y, num_classes=2)\n",
    "        history = self.model.fit(train_X, train_Y, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                                 validation_split=validation_split)\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "    def evaluate(self, test_X, test_Y):\n",
    "        test_Y = to_categorical(test_Y, num_classes=2)\n",
    "        score = self.model.evaluate(test_X, test_Y, verbose=0)\n",
    "        return score\n",
    "\n",
    "    def grid_search(self, param_grid, train_X, train_Y, test_X, test_Y, epochs=30, verbose=1):\n",
    "        best_model = None\n",
    "        best_score = -np.inf\n",
    "        best_params = None\n",
    "        best_history = None\n",
    "\n",
    "        param_combinations = ParameterGrid(param_grid)\n",
    "\n",
    "        for params in param_combinations:\n",
    "            print(f\"Training with parameters: {params}\")\n",
    "\n",
    "            self._build_model()\n",
    "            self.model.compile(\n",
    "                optimizer=params['optimizer'], loss=params['loss'], metrics=[\"accuracy\"])\n",
    "            history = self.train(train_X, train_Y, batch_size=params['batch_size'], epochs=epochs, validation_split=0.2)\n",
    "            score = self.evaluate(test_X, test_Y)\n",
    "\n",
    "            if score[1] > best_score:\n",
    "                best_model = self.model\n",
    "                best_score = score[1]\n",
    "                best_params = params\n",
    "                best_history = history\n",
    "\n",
    "            print(f\"Accuracy: {score[1]}\")\n",
    "\n",
    "        self.model = best_model\n",
    "\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best accuracy: {best_score}\")\n",
    "\n",
    "        return best_history, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "Here we load the training data that is going to be used across the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88419,
     "status": "ok",
     "timestamp": 1680151108620,
     "user": {
      "displayName": "Juan Carlos Wu (Jc)",
      "userId": "07576149222644747820"
     },
     "user_tz": 240
    },
    "id": "ncKPRAkIzrN_",
    "outputId": "bcc70ffb-7be6-47f1-dfb5-326a7c72f4d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1280, 224, 224, 3), (320, 224, 224, 3), (1280,), (320,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "train_X, test_X, train_Y, test_Y = load_data(random_state=random_seed, limit=400)\n",
    "train_X.shape, test_X.shape, train_Y.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Hardware Acceleration\n",
    "\n",
    "Since this notebook can also be run on Google Colab, we set this up such that it can use hardware acceleration if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1680151108621,
     "user": {
      "displayName": "Juan Carlos Wu (Jc)",
      "userId": "07576149222644747820"
     },
     "user_tz": 240
    },
    "id": "qv73GooM_q2n",
    "outputId": "c8678e98-f52e-4393-b147-62ee935bb90c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"CUDA is available. GPU:\", tf.config.list_physical_devices('GPU')[0])\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1680151108622,
     "user": {
      "displayName": "Juan Carlos Wu (Jc)",
      "userId": "07576149222644747820"
     },
     "user_tz": 240
    },
    "id": "IbQwBOTR0xpr"
   },
   "outputs": [],
   "source": [
    "epochs = 5 # keep the number of epochs when running locally, to save time. 5 might be too little to create anything good.\n",
    "param_grid_resnet = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'loss': ['categorical_crossentropy'],\n",
    "    'batch_size': [32, 64]\n",
    "}\n",
    "\n",
    "param_grid_vgg = {\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'loss': ['binary_crossentropy'],\n",
    "    'batch_size': [32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ws8xaKje2rkF",
    "outputId": "64b518c4-0037-4bb4-9d2b-edcc721a441c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'batch_size': 32, 'loss': 'binary_crossentropy', 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 77s 2s/step - loss: 0.5259 - accuracy: 0.7547 - val_loss: 0.4372 - val_accuracy: 0.8125\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 76s 2s/step - loss: 0.3678 - accuracy: 0.8453 - val_loss: 0.3125 - val_accuracy: 0.8719\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 78s 2s/step - loss: 0.2828 - accuracy: 0.8930 - val_loss: 0.2521 - val_accuracy: 0.8906\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9086"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m vgg_model \u001b[38;5;241m=\u001b[39m DetectorModelBaseVGG19()\n\u001b[1;32m----> 2\u001b[0m vgg_history, vgg_best_params \u001b[38;5;241m=\u001b[39m vgg_model\u001b[38;5;241m.\u001b[39mgrid_search(\n\u001b[0;32m      3\u001b[0m     param_grid_vgg, train_X, train_Y, test_X, test_Y, epochs\u001b[38;5;241m=\u001b[39mepochs)\n",
      "Cell \u001b[1;32mIn[34], line 65\u001b[0m, in \u001b[0;36mDetectorModelBaseVGG19.grid_search\u001b[1;34m(self, param_grid, train_X, train_Y, test_X, test_Y, epochs, verbose)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m], loss\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 65\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(train_X, train_Y, test_X, test_Y, batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], epochs\u001b[38;5;241m=\u001b[39mepochs, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m     67\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(test_X, test_Y)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m best_score:\n",
      "Cell \u001b[1;32mIn[34], line 26\u001b[0m, in \u001b[0;36mDetectorModelBaseVGG19.fit\u001b[1;34m(self, train_X, train_Y, test_X, test_Y, batch_size, epochs, verbose)\u001b[0m\n\u001b[0;32m     24\u001b[0m train_Y \u001b[38;5;241m=\u001b[39m to_categorical(train_Y, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses))\n\u001b[0;32m     25\u001b[0m test_Y \u001b[38;5;241m=\u001b[39m to_categorical(test_Y, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses))\n\u001b[1;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     27\u001b[0m             train_X,\n\u001b[0;32m     28\u001b[0m             train_Y,\n\u001b[0;32m     29\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     30\u001b[0m             epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m     31\u001b[0m             verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m     32\u001b[0m             validation_data\u001b[38;5;241m=\u001b[39m(test_X, test_Y))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\keras\\engine\\training.py:1729\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1715\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1716\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1717\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1727\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1728\u001b[0m     )\n\u001b[1;32m-> 1729\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m   1730\u001b[0m     x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1731\u001b[0m     y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   1732\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39mval_sample_weight,\n\u001b[0;32m   1733\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mvalidation_batch_size \u001b[38;5;129;01mor\u001b[39;00m batch_size,\n\u001b[0;32m   1734\u001b[0m     steps\u001b[38;5;241m=\u001b[39mvalidation_steps,\n\u001b[0;32m   1735\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1736\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[0;32m   1737\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[0;32m   1738\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[0;32m   1739\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1740\u001b[0m     _use_cached_eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1741\u001b[0m )\n\u001b[0;32m   1742\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1744\u001b[0m }\n\u001b[0;32m   1745\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\keras\\engine\\training.py:2072\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2070\u001b[0m ):\n\u001b[0;32m   2071\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2072\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_function(iterator)\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   2074\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    931\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    935\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\scoop\\apps\\miniconda3\\current\\envs\\cp322-project\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg_model = DetectorModelBaseVGG19()\n",
    "vgg_history, vgg_best_params = vgg_model.grid_search(\n",
    "    param_grid_vgg, train_X, train_Y, test_X, test_Y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vj0J7XyDalh"
   },
   "outputs": [],
   "source": [
    "resnet_model = DetectorModelBaseResNet()\n",
    "resnet_history, resnet_best_params = resnet_model.grid_search(\n",
    "    param_grid_resnet, train_X, train_Y, test_X, test_Y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpQ3kQY5IAwQ"
   },
   "outputs": [],
   "source": [
    "vgg_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcZwCxmMMDwD"
   },
   "outputs": [],
   "source": [
    "best_vgg_model = DetectorModelBaseVGG19()\n",
    "\n",
    "best_vgg_model.build()\n",
    "best_vgg_model.compile(optimizer=vgg_best_params['optimizer'], loss=vgg_best_params['loss'])\n",
    "\n",
    "best_vgg_model.fit(train_X, train_Y, test_X, test_Y, batch_size=vgg_best_params['batch_size'], epochs=10, verbose=0)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = load_data(target='test', random_state=35)\n",
    "\n",
    "score = best_vgg_model.score(test_X, test_Y)\n",
    "\n",
    "print(\"loss:\", score[0])\n",
    "print(\"accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFnAtDHAMs0O"
   },
   "outputs": [],
   "source": [
    "best_vgg_model.save(os.path.join(MODEL_SAVE_PATH, 'vgg19.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1680040725283,
     "user": {
      "displayName": "Juan Carlos Wu (Jc)",
      "userId": "07576149222644747820"
     },
     "user_tz": 240
    },
    "id": "gUSfxlbjOQNF",
    "outputId": "1497f5b9-06d3-46d5-cf86-91353d9b08ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32, 'loss': 'categorical_crossentropy', 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48776,
     "status": "ok",
     "timestamp": 1680041050633,
     "user": {
      "displayName": "Juan Carlos Wu (Jc)",
      "userId": "07576149222644747820"
     },
     "user_tz": 240
    },
    "id": "KtqNrfv6OwiN",
    "outputId": "4718cbd7-eef9-4572-d98a-a62cb7c3b3ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6/6 [==============================] - 6s 309ms/step - loss: 1.3128 - accuracy: 0.6708 - val_loss: 1.7005 - val_accuracy: 0.4878\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.9164 - accuracy: 0.7391 - val_loss: 1.9336 - val_accuracy: 0.6829\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 1.1939 - accuracy: 0.7019 - val_loss: 1.1193 - val_accuracy: 0.6098\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.4337 - accuracy: 0.8820 - val_loss: 0.9026 - val_accuracy: 0.7561\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.7655 - accuracy: 0.8447 - val_loss: 1.1338 - val_accuracy: 0.7561\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 1s 114ms/step - loss: 0.6707 - accuracy: 0.8385 - val_loss: 0.6090 - val_accuracy: 0.8293\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.4774 - accuracy: 0.8634 - val_loss: 0.6448 - val_accuracy: 0.8049\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.2862 - accuracy: 0.9255 - val_loss: 0.6668 - val_accuracy: 0.8049\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.1876 - accuracy: 0.9317 - val_loss: 0.6348 - val_accuracy: 0.8293\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.1565 - accuracy: 0.9503 - val_loss: 0.7853 - val_accuracy: 0.7317\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.2795 - accuracy: 0.9130 - val_loss: 0.7255 - val_accuracy: 0.7805\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1308 - accuracy: 0.9441 - val_loss: 0.7322 - val_accuracy: 0.7561\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.1309 - accuracy: 0.9627 - val_loss: 0.7598 - val_accuracy: 0.7561\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1399 - accuracy: 0.9379 - val_loss: 0.7675 - val_accuracy: 0.7561\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 0.0339 - accuracy: 0.9876 - val_loss: 0.7499 - val_accuracy: 0.7805\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.0854 - accuracy: 0.9689 - val_loss: 0.8954 - val_accuracy: 0.7073\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.0620 - accuracy: 0.9689 - val_loss: 0.7591 - val_accuracy: 0.7805\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.0617 - accuracy: 0.9814 - val_loss: 0.7935 - val_accuracy: 0.8049\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.7932 - val_accuracy: 0.7805\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.0339 - accuracy: 0.9938 - val_loss: 0.7720 - val_accuracy: 0.7805\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.0644 - accuracy: 0.9689 - val_loss: 0.7681 - val_accuracy: 0.7805\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 0.0374 - accuracy: 0.9876 - val_loss: 0.9159 - val_accuracy: 0.7073\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.0403 - accuracy: 0.9876 - val_loss: 0.8762 - val_accuracy: 0.8049\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 1s 107ms/step - loss: 0.0504 - accuracy: 0.9814 - val_loss: 0.8572 - val_accuracy: 0.7805\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 0.0349 - accuracy: 0.9876 - val_loss: 0.9026 - val_accuracy: 0.7561\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.0223 - accuracy: 0.9938 - val_loss: 0.7751 - val_accuracy: 0.8293\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.0748 - accuracy: 0.9689 - val_loss: 1.0257 - val_accuracy: 0.7317\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.0852 - accuracy: 0.9565 - val_loss: 0.7943 - val_accuracy: 0.8049\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 0.0366 - accuracy: 0.9814 - val_loss: 0.7813 - val_accuracy: 0.8293\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.9115 - val_accuracy: 0.7561\n",
      "loss: 0.2951255142688751\n",
      "accuracy: 0.9411764740943909\n"
     ]
    }
   ],
   "source": [
    "best_resnet_model = DetectorModelBaseResNet()\n",
    "best_resnet_model.model.compile(optimizer=resnet_best_params['optimizer'], loss=resnet_best_params['loss'], metrics=[\"accuracy\"])\n",
    "\n",
    "best_resnet_model.train(train_X, train_Y, batch_size=resnet_best_params['batch_size'], epochs=10, validation_split=0.2)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = load_data(target='test', random_state=35)\n",
    "\n",
    "score = best_resnet_model.evaluate(test_X, test_Y)\n",
    "\n",
    "print(\"loss:\", score[0])\n",
    "print(\"accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m_iasGXPR_U"
   },
   "outputs": [],
   "source": [
    "best_resnet_model.model.save(os.path.join(MODEL_SAVE_PATH, 'resnet.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxmT4lwiQLYk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMzbvIWEpEpLbl28I2pMnPs",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
